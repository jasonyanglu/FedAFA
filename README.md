# Personalized Federated Learning on Long-Tailed Data via Adversarial Feature Augmentation

This is the code for paper :  **Personalized Federated Learning on Long-Tailed Data via Adversarial Feature Augmentation**.

**Abstract**: Personalized Federated Learning (PFL) aims to learn personalized models on each client based on the knowledge across all clients in a privacy-preserving manner. Existing PFL methods generally assume that the underlying global data across all clients is uniformly distributed. However, the real-world data in more practical scenarios usually tends to exhibit long-tailed distributions. In this case, the tail classes are likely to be underrepresented due to their insufficient number of samples. At the same time, the existence of data heterogeneity further hampers the personalized model training, which makes the joint problem more challenging. To address the joint problem of data heterogeneity and long-tail distribution in PFL, we propose a PFL method called Federated Learning with Adversarial Feature Augmentation (FedAFA). FedAFA optimizes the personalized model on each client locally by training on a balanced feature set. The local minority class features are generated by transferring the knowledge extracted from the local majority class features by the global model. In this way, the PFL model inherits the power of the feature extractor of the global model and adapts to the local distribution by feature set rebalancing, such that it can generalize well on each client. Extensive experiments are conducted on several datasets (CIFAR-10/100, FMNIST) under different settings of data heterogeneity and long-tail distribution. The experimental results demonstrate that FedAFA not only significantly improves the performance of the local minority class compared to the state-of-the-art PFL algorithm. The code is temporarily available at https://anonymous.4open.science/r/FedAFA-7D63.

## Dependencies

* PyTorch >= 1.0.0

* torchvision >= 0.2.1

  

## Parameters

| Parameter     | Description                                              |
| ------------- | -------------------------------------------------------- |
| `dataset`     | Dataset to use. Options: `cifar10`,`cifar100`, `fmnist`. |
| `lr`          | Learning rate of model.                                  |
| `alpha`       | NonIIDness control. Option:`0.9,0.5,0.2`.                |
| `local_bs`    | Local batch size of training.                            |
| `test_bs`     | Test batch size .                                        |
| `num_users`   | Number of clients.                                       |
| `frac`        | the fraction of clients to be sampled in each round.     |
| `epochs`      | Number of communication rounds.                          |
| `local_ep`    | Number of local epochs.                                  |
| `imb_factor`  | Imbalanced control. Options: `0.01`.                     |
| `num_classes` | Number of classes.                                       |
| `gen_ep`      | Number of generation epoch.                              |
| `device`      | Specify the device to run the program.                   |
| `seed`        | The initial seed.                                        |
| `c`           | Balance factor.                                          |
| `p`           | Drop probability.                                        |


## Usage

Here is an example to run FedARN on CIFAR-10 with imb_fartor=0.01:

```
python main.py --dataset=cifar10 \
    --lr=0.01 \
    --alpha=0.2\
    --epochs=500\
    --local_ep=1 \
    --gen_ep=10\
    --num_users=20 \
    --num_classes=10 \
    --imb_factor=0.01\
    --c=0.6\
    --p=0.5
```

